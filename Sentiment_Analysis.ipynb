{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mWCmZcM_KRi"
   },
   "source": [
    "# Анализ тональности по отношению к выбранному объекту"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iOUJmfS_auK"
   },
   "source": [
    "Конечной целью исследования является отладка модели для определения тональности текста по отношению к выбранному объекту текста, например, к упомянутой в тексте персоне, организации и т.д. \n",
    "\n",
    "Такой анализ является более точным, чем стандартный анализ тональности, и представляет больший интерес для конечных пользователей, поскольку дает оценку отношения к конкретному объекту, а не ко всему тексту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK0Q-H5pAnfX"
   },
   "source": [
    "## Подготовка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qTTzDjapgr-p"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Установка окружения\n",
    "\n",
    "!pip install -q sklearn==0.22.2.post1\n",
    "!pip install mendelai-brat-parser==0.0.4\n",
    "!pip install smart_open==5.1.0\n",
    "!pip install tensorflow-text=2.5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YWVHhTq6gxJD"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Импорт библиотек\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from shutil import copyfile\n",
    "from brat_parser import get_entities_relations_attributes_groups\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # для загрузки universal-sentence-encoder-cmlm/multilingual-preprocess\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C2ROOsxkHnF8"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Определение рабочих директорий с данными\n",
    "SOURCE_DIR1 = 'done/'\n",
    "SOURCE_DIR2 = 'done1/'\n",
    "BASE_DIR = 'train_test/'\n",
    "main_csv_file = 'sent_quotes_done.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MX2mnYvA8KV"
   },
   "source": [
    "## Изучение и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZDe_LwrBQGE"
   },
   "source": [
    "Данные представлены в виде текстовых файлов и файлов аннотаций в формате BRAT.\n",
    "Все файлы находятся в двух папках done и done1. Файлы внутри папок не разделены по классам. Если в одном файле упоминается несколько объектов, по отношению к которым определяется тональность, данные находятся в одном файле ann.\n",
    "\n",
    "Для работы в Tensorflow датасет может быть представлен в двух форматах:\n",
    "1) как структурированный файл (например, csv), где каждая колонка является либо признаком, либо меткой класса;\n",
    "2) набор файлов, распределенных по директориям-классам. \n",
    "\n",
    "Кроме того, необходимо создать копии\n",
    "\n",
    "Подготовим данные для работы с Tensorflow\n",
    "(**для последующей работы с Google Colab в дальнейшем будет использоваться сохраненный файл csv, исполнение следующих двух ячеек не требуется**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "l-vixGELH5lN"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Функция для перевода данных в читаемый TensorFlow формат\n",
    "\n",
    "def files_to_df(source_dir, target_dir):\n",
    "    \"\"\"Собираем список файлов txt. \n",
    "    Идём по списку файлов, для каждого файла извлекаем текст сообщения (из txt), \n",
    "    текст объекта (ann), его индексы (ann), тональность (ann). \n",
    "    Раскладываем файлы на папки по классам, параллельно записываем в \n",
    "    dataframe pandas\"\"\"\n",
    "\n",
    "    if (target_dir[:-1] not in os.listdir()):\n",
    "      os.mkdir(target_dir)\n",
    "\n",
    "    file_names = [fn[:-4] for fn in os.listdir(source_dir) if fn[-3:]=='txt']\n",
    "    \n",
    "    text_to_process = []\n",
    "    \n",
    "    for fn in file_names:\n",
    "        txt_file_path = source_dir + fn +'.txt'\n",
    "        \n",
    "        with open(txt_file_path, encoding=\"utf8\") as f:\n",
    "            txt = f.read()\n",
    "        \n",
    "        ann_file_path = source_dir + fn +'.ann'\n",
    "        entities, relations, attributes, groups = get_entities_relations_attributes_groups(ann_file_path)\n",
    "        entities_keys = list(entities.keys())\n",
    "        \n",
    "        for key in entities_keys:\n",
    "            class_dir = entities[key].type\n",
    "            if (class_dir not in os.listdir(target_dir)):\n",
    "                os.mkdir(target_dir + class_dir)\n",
    "                \n",
    "            entity_id = entities[key].id\n",
    "\n",
    "            text_to_process.append({'filename':fn+'_'+entity_id, 'text': txt, 'entity_id': entities[key].id, \n",
    "                                    'entity_text': entities[key].text, 'entity_span': entities[key].span[0],\n",
    "                                   'label': entities[key].type})\n",
    "            \n",
    "            txt_new_file_path = target_dir + class_dir + '/' + fn + '_' + entity_id +'.txt'\n",
    "            copyfile(txt_file_path, txt_new_file_path)\n",
    "            ent_new_file_path = target_dir + class_dir + '/' + fn + '_' + entity_id + '_entity' +'.txt'\n",
    "            with open(ent_new_file_path, mode ='w', encoding =\"utf8\") as f:\n",
    "              f.write(entities[key].text)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(text_to_process)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-eIghZpYJ-OX"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Изменение структуры папок, перенос данных в csv\n",
    "\n",
    "df1 = files_to_df(SOURCE_DIR1, BASE_DIR)\n",
    "df2 = files_to_df(SOURCE_DIR2, BASE_DIR)\n",
    "df = pd.concat([df1, df2])\n",
    "df.to_csv(main_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcsmPtqlLaBY"
   },
   "source": [
    "Для работы в Google Colab требуется загрузка csv-файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "PhMVz0OVgzPa",
    "outputId": "6523194a-e091-4cbd-fd1c-dbf14ec32242"
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-484884b5af9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_csv_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 7 fields in line 2820, saw 8\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "#@title Чтение файл csv\n",
    "\n",
    "csv_file = main_csv_file\n",
    "dataframe = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylGTDnSSOdb-"
   },
   "source": [
    "Изучим распределение датасета по классам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Mij2NF5TWGvI"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Распределение классов\n",
    "\n",
    "dataframe.value_counts('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOAsFHEAWdKk"
   },
   "source": [
    "Классы несбалансированы. Класс Mixed_all состоит всего из 15 экземпляров. Укрупним классы, объединив классы с постфиксом _all с одноименными без префикса.\n",
    "Также приведем метки классов к числовым категориям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aDrs20INhOwg"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Объединение классов, приведение меток классов к категориальному формату\n",
    "\n",
    "classes = {'Positive': 0, 'Positive_all': 0, 'Negative': 1, 'Negative_all': 1,  \n",
    "           'Mixed': 2, 'Mixed_all': 2, 'Neutral': 3, 'Neutral_all': 3}\n",
    "dataframe['label'] = dataframe['type'].apply(lambda x: classes[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSOCZz0PXfv7"
   },
   "source": [
    "Разделим датасет на набор данных для тренировки и для тестирования.\n",
    "В тестовом датасете выделим набор для валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sgvSFWCklqE5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Объединение классов, приведение меток классов к категориальному формату\n",
    "\n",
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOIMMS-BX4S0"
   },
   "source": [
    "Поскольку классы не сбалансированы, проверим распределение по классам внутри тренировочного, тестового и валидационного датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0mkNVEu9YQ29",
    "outputId": "a4c5cdd0-cfbb-41ac-90d4-d2800838c58b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нормализованное распределение по классам в начальном датасете\n",
      "type\n",
      "3    0.759205\n",
      "1    0.127446\n",
      "0    0.107456\n",
      "2    0.005893\n",
      "dtype: float64\n",
      "Нормализованное распределение по классам в датасете train\n",
      "type\n",
      "3    0.757289\n",
      "1    0.128517\n",
      "0    0.108085\n",
      "2    0.006108\n",
      "dtype: float64\n",
      "Нормализованное распределение по классам в датасете test\n",
      "type\n",
      "3    0.757558\n",
      "1    0.128635\n",
      "0    0.108415\n",
      "2    0.005392\n",
      "dtype: float64\n",
      "Нормализованное распределение по классам в датасете val\n",
      "type\n",
      "3    0.768925\n",
      "1    0.121675\n",
      "0    0.103743\n",
      "2    0.005657\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "#@title Проверка распределения классов\n",
    "\n",
    "print('Нормализованное распределение по классам в начальном датасете')\n",
    "print(dataframe.value_counts('label', normalize=True))\n",
    "\n",
    "print('Нормализованное распределение по классам в датасете train')\n",
    "print(train.value_counts('label', normalize=True))\n",
    "\n",
    "print('Нормализованное распределение по классам в датасете test')\n",
    "print(test.value_counts('label', normalize=True))\n",
    "\n",
    "print('Нормализованное распределение по классам в датасете val')\n",
    "print(val.value_counts('label', normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26Mw9DQ2duiP"
   },
   "source": [
    "Распределение классов одинаково во всех наборах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNXTRHCadzVP"
   },
   "source": [
    "## Преобразование данных в датасет Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2TUXW0biw7G"
   },
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(train.label, num_classes=4)\n",
    "y_val = tf.keras.utils.to_categorical(val.label, num_classes=4)\n",
    "y_test = tf.keras.utils.to_categorical(test.label, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8oADJZ0zzu9",
    "outputId": "2d4f88ab-1a84-4d27-82e7-f895a3d629f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['text', 'entity_text']\n",
      "A batch of targets: tf.Tensor([3 0 3 3 3], shape=(5,), dtype=int64)\n",
      "A batch of features (text): tf.Tensor(\n",
      "[b'\\xd0\\x9d\\xd0\\xbe \\xd1\\x82\\xd0\\xbe\\xd0\\xb3\\xd0\\xb4\\xd0\\xb0, \\xd0\\x9d\\xd0\\xb8\\xd0\\xba\\xd0\\xb8\\xd1\\x82\\xd0\\xb0 \\xd0\\xa1\\xd0\\xb5\\xd1\\x80\\xd0\\xb3\\xd0\\xb5\\xd0\\xb5\\xd0\\xb2\\xd0\\xb8\\xd1\\x87, \\xd1\\x8f \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb7\\xd1\\x8b\\xd0\\xb2\\xd0\\xb0\\xd1\\x8e \\xd0\\xb2\\xd0\\xb0\\xd1\\x81 \\xd0\\xb1\\xd1\\x8b\\xd1\\x82\\xd1\\x8c \\xd0\\xbf\\xd0\\xbe\\xd1\\x81\\xd0\\xbb\\xd0\\xb5\\xd0\\xb4\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd1\\x82\\xd0\\xb5\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd1\\x8b\\xd0\\xbc. \\xd0\\x92\\xd0\\xb5\\xd1\\x80\\xd0\\xbd\\xd0\\xb8\\xd1\\x82\\xd0\\xb5 \\xd0\\xb2\\xd0\\xb0\\xd1\\x88\\xd0\\xb5\\xd0\\xb3\\xd0\\xbe \\xd0\\x9e\\xd1\\x81\\xd0\\xba\\xd0\\xb0\\xd1\\x80\\xd0\\xb0 \\xd0\\xbc\\xd0\\xb5\\xd1\\x80\\xd0\\xb7\\xd0\\xba\\xd0\\xb8\\xd0\\xbc \\xd0\\xb0\\xd0\\xbc\\xd0\\xb5\\xd1\\x80\\xd0\\xb8\\xd0\\xba\\xd0\\xb0\\xd0\\xbd\\xd1\\x86\\xd0\\xb0\\xd0\\xbc, \\xd0\\xba\\xd0\\xbe\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd1\\x8b\\xd0\\xb5 \"\\xd0\\xb1\\xd0\\xbe\\xd0\\xbc\\xd0\\xb1\\xd1\\x8f\\xd1\\x82 \\xd0\\x98\\xd1\\x80\\xd0\\xb0\\xd0\\xba \\xd0\\xb8 \\xd0\\x9b\\xd0\\xb8\\xd0\\xb2\\xd0\\xb8\\xd1\\x8e\" \\xd0\\xb8 \"\\xd1\\x82\\xd0\\xb5\\xd1\\x80\\xd0\\xb7\\xd0\\xb0\\xd1\\x8e\\xd1\\x82\" \\xd1\\x80\\xd1\\x83\\xd0\\xba\\xd0\\xbe\\xd0\\xb2\\xd0\\xbe\\xd0\\xb4\\xd0\\xb8\\xd1\\x82\\xd0\\xb5\\xd0\\xbb\\xd0\\xb5\\xd0\\xb9 \\xd0\\xb4\\xd1\\x80\\xd1\\x83\\xd0\\xb3\\xd0\\xb8\\xd1\\x85 \\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd0\\xb0\\xd0\\xbd! \\xd0\\x92\\xd1\\x8b \\xd0\\xb6\\xd0\\xb5 \\xd0\\xb8\\xd1\\x85 \\xd0\\xbd\\xd0\\xb5 \\xd1\\x83\\xd0\\xb2\\xd0\\xb0\\xd0\\xb6\\xd0\\xb0\\xd0\\xb5\\xd1\\x82\\xd0\\xb5? \\xd0\\x90 \\xd0\\xb7\\xd0\\xb0\\xd0\\xbe\\xd0\\xb4\\xd0\\xbd\\xd0\\xbe \\xd0\\xb8 \"\\xd0\\xb3\\xd0\\xb5\\xd0\\xb9\\xd1\\x80\\xd0\\xbe\\xd0\\xbf\\xd0\\xb5\" \\xd0\\xb2\\xd0\\xb5\\xd1\\x80\\xd0\\xbd\\xd0\\xb8\\xd1\\x82\\xd0\\xb5 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb7\\xd1\\x8b \\xd0\\xb8\\xd0\\xb7 \\xd0\\x98\\xd1\\x81\\xd0\\xbf\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd0\\xb8, \\xd0\\xa4\\xd1\\x80\\xd0\\xb0\\xd0\\xbd\\xd1\\x86\\xd0\\xb8\\xd0\\xb8, \\xd0\\x98\\xd1\\x82\\xd0\\xb0\\xd0\\xbb\\xd0\\xb8\\xd0\\xb8 \\xd0\\xb2 \\xd0\\xbe\\xd1\\x82\\xd0\\xb2\\xd0\\xb5\\xd1\\x82 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xb8\\xd1\\x85 \\xd1\\x81\\xd0\\xb0\\xd0\\xbd\\xd0\\xba\\xd1\\x86\\xd0\\xb8\\xd0\\xb8'\n",
      " b'\\xd0\\x9e\\xd1\\x88\\xd0\\xb8\\xd0\\xb1\\xd0\\xba\\xd0\\xb0 \\xd0\\x94\\xd0\\xb5 \\xd0\\xa5\\xd0\\xb5\\xd0\\xb0 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8 \\xd0\\xb2\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd0\\xbc \\xd0\\xb3\\xd0\\xbe\\xd0\\xbb\\xd0\\xb5? \\xd0\\x92\\xd1\\x81\\xd0\\xb5 \\xd1\\x81\\xd1\\x82\\xd0\\xb0\\xd0\\xbb\\xd0\\xba\\xd0\\xb8\\xd0\\xb2\\xd0\\xb0\\xd1\\x8e\\xd1\\x82\\xd1\\x81\\xd1\\x8f \\xd1\\x81 \\xd1\\x8d\\xd1\\x82\\xd0\\xb8\\xd0\\xbc. \\xd0\\xaf \\xd1\\x83\\xd0\\xb2\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xbd, \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd0\\xb2 \\xd1\\x81\\xd0\\xb0\\xd0\\xbc\\xd1\\x8b\\xd0\\xb5 \\xd0\\xb2\\xd0\\xb0\\xd0\\xb6\\xd0\\xbd\\xd1\\x8b\\xd0\\xb5 \\xd0\\xbc\\xd0\\xbe\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd1\\x82\\xd1\\x8b \\xd0\\xbe\\xd0\\xbd \\xd0\\xb1\\xd1\\x83\\xd0\\xb4\\xd0\\xb5\\xd1\\x82 \\xd1\\x82\\xd0\\xb0\\xd0\\xba\\xd0\\xb8\\xd0\\xbc, \\xd0\\xba\\xd0\\xb0\\xd0\\xba\\xd0\\xb8\\xd0\\xbc \\xd0\\xbc\\xd1\\x8b \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb2\\xd1\\x8b\\xd0\\xba\\xd0\\xbb\\xd0\\xb8 \\xd0\\xb2\\xd1\\x81\\xd0\\xb5\\xd0\\xb3\\xd0\\xb4\\xd0\\xb0 \\xd0\\xb5\\xd0\\xb3\\xd0\\xbe \\xd0\\xb2\\xd0\\xb8\\xd0\\xb4\\xd0\\xb5\\xd1\\x82\\xd1\\x8c'\n",
      " b'\\xd0\\x98\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd1\\x87\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd0\\xbe \\xd0\\xb7\\xd0\\xb0 \\xd0\\xb1\\xd0\\xbe\\xd0\\xb9 \\xd0\\xa2\\xd0\\xb8\\xd1\\x82\\xd0\\xbe\\xd0\\xb2 \\xd0\\xb4\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd1\\x80\\xd0\\xb8\\xd0\\xb2\\xd0\\xb0\\xd0\\xbb\\xd1\\x81\\xd1\\x8f. \\xd0\\x9f\\xd0\\xbe\\xd1\\x82\\xd0\\xbe\\xd0\\xbc \\xd0\\x9a\\xd0\\xb8\\xd1\\x80\\xd0\\xb8\\xd0\\xbb\\xd0\\xbb \\xd0\\xa9\\xd0\\xb5\\xd0\\xba\\xd1\\x83\\xd1\\x82\\xd1\\x8c\\xd0\\xb5\\xd0\\xb2 (\\xd0\\xbe\\xd1\\x82 \\xd0\\xa4\\xd0\\x91\\xd0\\xa0 \\xe2\\x80\\x94 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xbc. \\xd1\\x80\\xd0\\xb5\\xd0\\xb4.) \\xd0\\xbe\\xd0\\xb1\\xd0\\xb5\\xd1\\x89\\xd0\\xb0\\xd0\\xbb \\xd0\\xb4\\xd0\\xb5\\xd0\\xbd\\xd1\\x8c\\xd0\\xb3\\xd0\\xb8 \\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb2\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb8, \\xd0\\xb8 \\xd0\\xb2\\xd1\\x81\\xd0\\xb5 \\xd1\\x80\\xd0\\xb5\\xd0\\xba\\xd0\\xb2\\xd0\\xb8\\xd0\\xb7\\xd0\\xb8\\xd1\\x82\\xd1\\x8b \\xd0\\xba\\xd0\\xb0\\xd1\\x80\\xd1\\x82\\xd1\\x8b \\xd1\\x8f \\xd0\\xb5\\xd0\\xbc\\xd1\\x83 \\xd0\\xbe\\xd1\\x82\\xd0\\xbf\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xbb\\xd1\\x8f\\xd0\\xbb. \\xd0\\x98\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd0\\xbe \\xd0\\x9a\\xd0\\xb8\\xd1\\x80\\xd0\\xb8\\xd0\\xbb\\xd0\\xbb \\xd0\\xb4\\xd0\\xbe\\xd0\\xbb\\xd0\\xb6\\xd0\\xb5\\xd0\\xbd \\xd0\\xb1\\xd1\\x8b\\xd0\\xbb \\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb2\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb8 \\xd0\\xb4\\xd0\\xb5\\xd0\\xbd\\xd1\\x8c\\xd0\\xb3\\xd0\\xb8\\xc2\\xbb '\n",
      " b'\\xd0\\x9c\\xd0\\xb0\\xd1\\x88\\xd0\\xb8\\xd0\\xbd\\xd0\\xb0 \\xd0\\xbf\\xd0\\xb5\\xd0\\xb2\\xd0\\xb8\\xd1\\x86\\xd1\\x8b \\xd0\\x90\\xd0\\xbb\\xd0\\xbb\\xd1\\x8b \\xd0\\x9f\\xd1\\x83\\xd0\\xb3\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd0\\xb2\\xd0\\xbe\\xd0\\xb9 \\xd0\\xb4\\xd0\\xb2\\xd0\\xb8\\xd0\\xb3\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0\\xd1\\x81\\xd1\\x8c \\xd0\\xbf\\xd0\\xbe \\xd1\\x82\\xd0\\xb5\\xd1\\x85\\xd0\\xbd\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd0\\xb9 \\xd0\\xbf\\xd0\\xbb\\xd0\\xb0\\xd1\\x82\\xd1\\x84\\xd0\\xbe\\xd1\\x80\\xd0\\xbc\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xa0\\xd0\\xb8\\xd0\\xb6\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd0\\xbc \\xd0\\xb2\\xd0\\xbe\\xd0\\xba\\xd0\\xb7\\xd0\\xb0\\xd0\\xbb\\xd0\\xb5 \\xd0\\xb2 \\xd0\\x9c\\xd0\\xbe\\xd1\\x81\\xd0\\xba\\xd0\\xb2\\xd0\\xb5 \\xd0\\xb8, \\xd0\\xbf\\xd0\\xbe\\xd1\\x8d\\xd1\\x82\\xd0\\xbe\\xd0\\xbc\\xd1\\x83, \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbc\\xd0\\xb5\\xd1\\x88\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0 \\xd0\\xbf\\xd0\\xb0\\xd1\\x81\\xd1\\x81\\xd0\\xb0\\xd0\\xb6\\xd0\\xb8\\xd1\\x80\\xd0\\xb0\\xd0\\xbc, \\xd0\\xbd\\xd0\\xbe \\xd0\\xba \\xd0\\xbf\\xd0\\xbe\\xd0\\xb5\\xd0\\xb7\\xd0\\xb4\\xd0\\xb0\\xd0\\xbc \\xd1\\x80\\xd0\\xb0\\xd0\\xb7\\xd1\\x80\\xd0\\xb5\\xd1\\x88\\xd0\\xb5\\xd0\\xbd\\xd0\\xbe \\xd0\\xbf\\xd0\\xbe\\xd0\\xb4\\xd1\\x8a\\xd0\\xb5\\xd0\\xb7\\xd0\\xb6\\xd0\\xb0\\xd1\\x82\\xd1\\x8c \\xd1\\x82\\xd0\\xbe\\xd0\\xbb\\xd1\\x8c\\xd0\\xba\\xd0\\xbe \\xd1\\x81\\xd0\\xbf\\xd0\\xb5\\xd1\\x86\\xd0\\xb8\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd0\\xbe\\xd0\\xb9 \\xd1\\x82\\xd0\\xb5\\xd1\\x85\\xd0\\xbd\\xd0\\xb8\\xd0\\xba\\xd0\\xb5, \\xd0\\xbf\\xd0\\xbe\\xd1\\x8d\\xd1\\x82\\xd0\\xbe\\xd0\\xbc\\xd1\\x83 \\xd0\\xb4\\xd0\\xb5\\xd0\\xb9\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xb8\\xd1\\x8f \\xd1\\x80\\xd0\\xb0\\xd0\\xb1\\xd0\\xbe\\xd1\\x82\\xd0\\xbd\\xd0\\xb8\\xd0\\xba\\xd0\\xbe\\xd0\\xb2 \\xd0\\xb2\\xd0\\xbe\\xd0\\xba\\xd0\\xb7\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb2\\xd0\\xb5\\xd1\\x80\\xd1\\x8f\\xd1\\x82\\xc2\\xbb '\n",
      " b'\\xd0\\xa1\\xd0\\xbe\\xd0\\xb2\\xd0\\xb5\\xd1\\x80\\xd1\\x88\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd0\\xbe \\xd0\\xbe\\xd1\\x87\\xd0\\xb5\\xd0\\xb2\\xd0\\xb8\\xd0\\xb4\\xd0\\xbd\\xd0\\xbe, \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd0\\xa2\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb7\\xd0\\xb0 \\xd0\\x9c\\xd1\\x8d\\xd0\\xb9 \\xd1\\x81\\xd0\\xb4\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0 \\xd0\\xb2\\xd1\\x81\\xd0\\xb5 \\xd0\\xbf\\xd1\\x80\\xd0\\xb5\\xd0\\xb6\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd0\\xbf\\xd0\\xbe\\xd0\\xb7\\xd0\\xb8\\xd1\\x86\\xd0\\xb8\\xd0\\xb8. \\xd0\\x95\\xd0\\xb5 \\xd0\\xbc\\xd0\\xb8\\xd0\\xbd\\xd0\\xb8\\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd1\\x8b \\xd0\\xb2\\xd1\\x8b\\xd0\\xbf\\xd1\\x80\\xd1\\x8b\\xd0\\xb3\\xd0\\xb8\\xd0\\xb2\\xd0\\xb0\\xd1\\x8e\\xd1\\x82 \\xd0\\xb7\\xd0\\xb0 \\xd0\\xb1\\xd0\\xbe\\xd1\\x80\\xd1\\x82, \\xd0\\xb0 \\xd1\\x81\\xd0\\xb0\\xd0\\xbc\\xd0\\xbe \\xd0\\xbf\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xb8\\xd1\\x82\\xd0\\xb5\\xd0\\xbb\\xd1\\x8c\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe \\xd1\\x82\\xd0\\xbe\\xd0\\xbd\\xd0\\xb5\\xd1\\x82'], shape=(5,), dtype=string)\n",
      "A batch of features (entity_text): tf.Tensor(\n",
      "[b'\\xd0\\x9d\\xd0\\xb8\\xd0\\xba\\xd0\\xb8\\xd1\\x82\\xd0\\xb0 \\xd0\\xa1\\xd0\\xb5\\xd1\\x80\\xd0\\xb3\\xd0\\xb5\\xd0\\xb5\\xd0\\xb2\\xd0\\xb8\\xd1\\x87'\n",
      " b'\\xd0\\x94\\xd0\\xb5 \\xd0\\xa5\\xd0\\xb5\\xd0\\xb0'\n",
      " b'\\xd0\\xa2\\xd0\\xb8\\xd1\\x82\\xd0\\xbe\\xd0\\xb2'\n",
      " b'\\xd0\\x90\\xd0\\xbb\\xd0\\xbb\\xd1\\x8b \\xd0\\x9f\\xd1\\x83\\xd0\\xb3\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd0\\xb2\\xd0\\xbe\\xd0\\xb9'\n",
      " b'\\xd0\\xa2\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb7\\xd0\\xb0 \\xd0\\x9c\\xd1\\x8d\\xd0\\xb9'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('type')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds\n",
    "\n",
    "batch_size = 5\n",
    "train_ds = df_to_dataset(train[['text', 'entity_text','type']], batch_size=batch_size)\n",
    "\n",
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print('Every feature:', list(train_features.keys()))\n",
    "print('A batch of targets:', label_batch )\n",
    "print('A batch of features (text):',train_features['text'])\n",
    "print('A batch of features (entity_text):',train_features['entity_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3tc82FsT-pN"
   },
   "source": [
    "## Выбор модели энкодера\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL8n0cV9aSW1"
   },
   "source": [
    "Для работы выбрана модель [LaBSE](https://tfhub.dev/google/LaBSE/2) (Language-agnostic BERT sentence embedding model) как одна из наиболее актуальных моделей, показывающих хорошие результаты для русского языка.\n",
    "\n",
    "Данные для этого энкодера должны быть предварительно обработаны препроцессором [universal-sentence-encoder-cmlm/multilingual-preprocess](https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SV6Bg11IcWrm"
   },
   "outputs": [],
   "source": [
    "labse_preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "labse_encoder = hub.KerasLayer(\"https://tfhub.dev/google/LaBSE/2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBm2J6mvejtm"
   },
   "source": [
    "## Изменение стандартного препроцессора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRfr7LdrevJ7"
   },
   "source": [
    "Стандартный препроцессор получает на вход текст и выдает в качестве output словарь из трех тензоров:  \n",
    "- `input_word_ids`: id поданных на вход слов  \n",
    "- `input_mask`: маска из 1 и 0, где 1 находится на позициях значимых слов, 0 на позициях паддинга  \n",
    "- `input_type_ids`: маска для передачи дополнительной информации о токенах.\n",
    "\n",
    "Нам необходимо изменить препроцессор таким образом, что в маске `input_type_ids` на позициях токенов интересующего нас объекта стояли 1, на всех остальных позициях - 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73RdoDBfhEZd"
   },
   "outputs": [],
   "source": [
    "preprocessor = hub.load(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\")\n",
    "\n",
    "# Step 1: tokenize batches of text inputs.\n",
    "text_inputs = [tf.keras.layers.Input(shape=(), dtype=tf.string),\n",
    "               ...] # This SavedModel accepts up to 2 text inputs.\n",
    "tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
    "tokenized_inputs = [tokenize(segment) for segment in text_inputs]\n",
    "\n",
    "# Step 2 (optional): modify tokenized inputs.\n",
    "pass\n",
    "\n",
    "# Step 3: pack input sequences for the Transformer encoder.\n",
    "seq_length = 128  # Your choice here.\n",
    "bert_pack_inputs = hub.KerasLayer(\n",
    "    preprocessor.bert_pack_inputs,\n",
    "    arguments=dict(seq_length=seq_length))  # Optional argument.\n",
    "encoder_inputs = bert_pack_inputs(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57YquaLtVmxx"
   },
   "outputs": [],
   "source": [
    "text_test = ['\"да, я допустил неточность, а посол Мюррей ссылается на слухи... но зато теперь мы накопали много РЕАЛЬНОЙ информации на Усманова']\n",
    "token_text = ['Мюррей']\n",
    "text_preprocessed = preprocessor(text_test)\n",
    "token_preprocessed = preprocessor(token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlcSOOyTuNaJ"
   },
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "           \n",
    "def findFirst_numpy(a, b):\n",
    "    temp = rolling_window(a, len(b))\n",
    "    result = np.where(np.all(temp == b, axis=1))\n",
    "    return result[0][0] if result else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxM3LVCOWwLp"
   },
   "outputs": [],
   "source": [
    "def change_typeid(text_preprocessed, token_text):\n",
    "  \n",
    "  token_preprocessed = preprocessor(token_text)\n",
    "  token_numpy = token_preprocessed['input_word_ids'].numpy()[0]\n",
    "  start = 1\n",
    "  end = np.where(token_numpy == 102)[0][0]\n",
    "  token_codes = token_numpy[start:end]\n",
    "\n",
    "  text_numpy = text_preprocessed['input_word_ids'].numpy()[0]\n",
    "\n",
    "  indx_start = findFirst_numpy(text_numpy, token_codes)\n",
    "  indx_end = indx_start+len(token_codes)\n",
    "\n",
    "  pattern_to_modify = text_preprocessed[\"input_type_ids\"][0].numpy()\n",
    "  pattern_to_modify[indx_start: indx_end] = 1\n",
    "  pattern_to_modify = [pattern_to_modify]\n",
    "\n",
    "  text_preprocessed[\"input_type_ids\"] = tf.add (text_preprocessed[\"input_type_ids\"], pattern_to_modify)\n",
    "  return pattern_to_modify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_Ap4ilLq8M2",
    "outputId": "e309b053-6a5c-4fb6-f4a2-358ffa701bfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :30]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUKVKajy2-4G",
    "outputId": "bb99299a-f6c9-42d5-cd41-3c5a77b72928"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)]"
      ]
     },
     "execution_count": 144,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_typeid(text_preprocessed, token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gGokwNrIjs-5",
    "outputId": "d8a9b6e7-485d-4770-df10-eb9118abf15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   677 162813 108855]\n",
      "13\n",
      "16\n",
      "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jm6Rf2yPlrxw",
    "outputId": "3cff2ab2-fa84-4b3c-e790-5247cf68f76b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 145,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessed[\"input_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHyahYdsnCuB"
   },
   "outputs": [],
   "source": [
    "bert_results = encoder(text_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN0rZjAJxe2P"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = labse_preprocessor \n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = labse_encoder\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9EYxg1XpeKA",
    "outputId": "1d61b2d6-4bfd-47c5-fc9b-bf4c4cb6e400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.48670408]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhxFSL1ozegn"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "-lFXK0bLzfNz",
    "outputId": "5d45d0b3-b23b-405d-98f4-f70db352ee73"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-ccf87087df35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_train_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOTWj4ILzhsi",
    "outputId": "aeb60b94-0ca5-4a39-f53a-9376dbd5b5be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[37 12 25  5  9 20 21  0  0]\n",
      " [51 34 27 33 29 18  0  0  0]\n",
      " [49 52 30 31 19 46 10  0  0]\n",
      " [ 7  5 50 43 28  7 47 17  0]\n",
      " [24 35 39 40  3  6 32 16  0]\n",
      " [ 4  2 15 14 22 23  0  0  0]\n",
      " [36 48  6 38 42  3 45  0  0]\n",
      " [ 4  2 13 41 53  8 44 26 11]], shape=(8, 9), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "data = [\n",
    "    \"ξεῖν᾽, ἦ τοι μὲν ὄνειροι ἀμήχανοι ἀκριτόμυθοι\",\n",
    "    \"γίγνοντ᾽, οὐδέ τι πάντα τελείεται ἀνθρώποισι.\",\n",
    "    \"δοιαὶ γάρ τε πύλαι ἀμενηνῶν εἰσὶν ὀνείρων:\",\n",
    "    \"αἱ μὲν γὰρ κεράεσσι τετεύχαται, αἱ δ᾽ ἐλέφαντι:\",\n",
    "    \"τῶν οἳ μέν κ᾽ ἔλθωσι διὰ πριστοῦ ἐλέφαντος,\",\n",
    "    \"οἵ ῥ᾽ ἐλεφαίρονται, ἔπε᾽ ἀκράαντα φέροντες:\",\n",
    "    \"οἱ δὲ διὰ ξεστῶν κεράων ἔλθωσι θύραζε,\",\n",
    "    \"οἵ ῥ᾽ ἔτυμα κραίνουσι, βροτῶν ὅτε κέν τις ἴδηται.\",\n",
    "]\n",
    "layer = preprocessing.TextVectorization()\n",
    "layer.adapt(data)\n",
    "vectorized_text = layer(data)\n",
    "print(vectorized_text)\n",
    "\n",
    "StringLookup"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
