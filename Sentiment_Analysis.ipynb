{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6x+MQm83gaTnZauzrq0AF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shatokua/sent_analysis/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mWCmZcM_KRi"
      },
      "source": [
        "# Анализ тональности по отношению к выбранному объекту"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iOUJmfS_auK"
      },
      "source": [
        "Конечной целью исследования является отладка модели для определения тональности текста по отношению к выбранному объекту текста, например, к упомянутой в тексте персоне, организации и т.д. \n",
        "\n",
        "Такой анализ является более точным, чем стандартный анализ тональности, и представляет больший интерес для конечных пользователей, поскольку дает оценку отношения к конкретному объекту, а не ко всему тексту."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK0Q-H5pAnfX"
      },
      "source": [
        "## Подготовка окружения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTTzDjapgr-p"
      },
      "source": [
        "%%capture\n",
        "#@title Установка окружения\n",
        "\n",
        "!pip install -q sklearn==0.22.2.post1\n",
        "!pip install mendelai-brat-parser==0.0.4\n",
        "!pip install smart_open==5.1.0\n",
        "!pip install tensorflow-text==2.5.0\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWVHhTq6gxJD"
      },
      "source": [
        "%%capture\n",
        "#@title Импорт библиотек\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from shutil import copyfile\n",
        "from brat_parser import get_entities_relations_attributes_groups\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text  # для загрузки universal-sentence-encoder-cmlm/multilingual-preprocess\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ROOsxkHnF8"
      },
      "source": [
        "%%capture\n",
        "#@title Определение рабочих директорий с данными\n",
        "SOURCE_DIR1 = 'done/'\n",
        "SOURCE_DIR2 = 'done1/'\n",
        "BASE_DIR = 'train_test/'\n",
        "main_csv_file = 'sent_quotes_done.csv'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MX2mnYvA8KV"
      },
      "source": [
        "## Изучение и подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZDe_LwrBQGE"
      },
      "source": [
        "Данные представлены в виде текстовых файлов и файлов аннотаций в формате BRAT.\n",
        "Все файлы находятся в двух папках done и done1. Файлы внутри папок не разделены по классам. Если в одном файле упоминается несколько объектов, по отношению к которым определяется тональность, данные находятся в одном файле ann.\n",
        "\n",
        "Для работы в Tensorflow датасет может быть представлен в двух форматах:\n",
        "1) как структурированный файл (например, csv), где каждая колонка является либо признаком, либо меткой класса;\n",
        "2) набор файлов, распределенных по директориям-классам. \n",
        "\n",
        "Кроме того, необходимо создать копии\n",
        "\n",
        "Подготовим данные для работы с Tensorflow\n",
        "(**для последующей работы с Google Colab в дальнейшем будет использоваться сохраненный файл csv, исполнение следующих двух ячеек не требуется**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-vixGELH5lN"
      },
      "source": [
        "%%capture\n",
        "#@title Функция для перевода данных в читаемый TensorFlow формат\n",
        "\n",
        "def files_to_df(source_dir, target_dir):\n",
        "    \"\"\"Собираем список файлов txt. \n",
        "    Идём по списку файлов, для каждого файла извлекаем текст сообщения (из txt), \n",
        "    текст объекта (ann), его индексы (ann), тональность (ann). \n",
        "    Раскладываем файлы на папки по классам, параллельно записываем в \n",
        "    dataframe pandas\"\"\"\n",
        "\n",
        "    if (target_dir[:-1] not in os.listdir()):\n",
        "      os.mkdir(target_dir)\n",
        "\n",
        "    file_names = [fn[:-4] for fn in os.listdir(source_dir) if fn[-3:]=='txt']\n",
        "    \n",
        "    text_to_process = []\n",
        "    \n",
        "    for fn in file_names:\n",
        "        txt_file_path = source_dir + fn +'.txt'\n",
        "        \n",
        "        with open(txt_file_path, encoding=\"utf8\") as f:\n",
        "            txt = f.read()\n",
        "        \n",
        "        ann_file_path = source_dir + fn +'.ann'\n",
        "        entities, relations, attributes, groups = get_entities_relations_attributes_groups(ann_file_path)\n",
        "        entities_keys = list(entities.keys())\n",
        "        \n",
        "        for key in entities_keys:\n",
        "            class_dir = entities[key].type\n",
        "            if (class_dir not in os.listdir(target_dir)):\n",
        "                os.mkdir(target_dir + class_dir)\n",
        "                \n",
        "            entity_id = entities[key].id\n",
        "            print(type(entities[key].span[0][0]))\n",
        "\n",
        "            text_to_process.append({'filename':fn+'_'+entity_id, 'text': txt, 'entity_id': entities[key].id, \n",
        "                                    'entity_text': entities[key].text, 'entity_span_start': entities[key].span[0][0],\n",
        "                                    'entity_span_end': entities[key].span[0][1], 'label': entities[key].type})\n",
        "            \n",
        "            txt_new_file_path = target_dir + class_dir + '/' + fn + '_' + entity_id +'.txt'\n",
        "            copyfile(txt_file_path, txt_new_file_path)\n",
        "            ent_new_file_path = target_dir + class_dir + '/' + fn + '_' + entity_id + '_entity' +'.txt'\n",
        "            with open(ent_new_file_path, mode ='w', encoding =\"utf8\") as f:\n",
        "              f.write(entities[key].text)\n",
        "    \n",
        "    df = pd.DataFrame.from_dict(text_to_process)\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eIghZpYJ-OX",
        "outputId": "9997947a-e064-4c6c-fb29-52f5359a18cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "%%capture\n",
        "#@title Изменение структуры папок, перенос данных в csv\n",
        "\n",
        "df1 = files_to_df(SOURCE_DIR1, BASE_DIR)\n",
        "df2 = files_to_df(SOURCE_DIR2, BASE_DIR)\n",
        "df = pd.concat([df1, df2])\n",
        "df.to_csv(main_csv_file)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fca0a7778e06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Изменение структуры папок, перенос данных в csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSOURCE_DIR1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSOURCE_DIR2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'files_to_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcsmPtqlLaBY"
      },
      "source": [
        "Для работы в Google Colab требуется загрузка csv-файла."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhMVz0OVgzPa"
      },
      "source": [
        "%%capture\n",
        "#@title Чтение файл csv\n",
        "\n",
        "csv_file = main_csv_file\n",
        "dataframe = pd.read_csv(csv_file)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylGTDnSSOdb-"
      },
      "source": [
        "Изучим распределение датасета по классам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEgvHhZQ-cHF"
      },
      "source": [
        "%%capture\n",
        "#@title Распределение классов\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjLWDdHN-tQV",
        "outputId": "44354720-da1c-4dc3-d3f4-c09c6078923d"
      },
      "source": [
        "print(dataframe.value_counts('label'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label\n",
            "Neutral_all     29070\n",
            "Neutral         10354\n",
            "Negative         4836\n",
            "Positive         4657\n",
            "Negative_all     1782\n",
            "Positive_all      923\n",
            "Mixed             291\n",
            "Mixed_all          15\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOAsFHEAWdKk"
      },
      "source": [
        "Классы несбалансированы. Класс Mixed_all состоит всего из 15 экземпляров. Укрупним классы, объединив классы с постфиксом _all с одноименными без префикса.\n",
        "Также приведем метки классов к числовым категориям"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDrs20INhOwg"
      },
      "source": [
        "%%capture\n",
        "#@title Объединение классов, приведение меток классов к категориальному формату\n",
        "\n",
        "classes = {'Positive': 0, 'Positive_all': 0, 'Negative': 1, 'Negative_all': 1,  \n",
        "           'Mixed': 2, 'Mixed_all': 2, 'Neutral': 3, 'Neutral_all': 3}\n",
        "dataframe['label'] = dataframe['label'].apply(lambda x: classes[x])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSOCZz0PXfv7"
      },
      "source": [
        "Разделим датасет на набор данных для тренировки и для тестирования.\n",
        "В тренировочном датасете выделим набор для валидации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgvSFWCklqE5"
      },
      "source": [
        "%%capture\n",
        "#@title Объединение классов, приведение меток классов к категориальному формату\n",
        "\n",
        "train, test = train_test_split(dataframe, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzmsqxAP-1EX",
        "outputId": "3e2080c8-edbb-4efd-b587-23acc03e656a"
      },
      "source": [
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33233 train examples\n",
            "8309 validation examples\n",
            "10386 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOIMMS-BX4S0"
      },
      "source": [
        "Поскольку классы не сбалансированы, проверим распределение по классам внутри тренировочного, тестового и валидационного датасетов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mkNVEu9YQ29"
      },
      "source": [
        "%%capture\n",
        "#@title Проверка распределения классов\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uo2fQAH-6M9",
        "outputId": "1349f40a-0db1-4999-b0ec-43102e612a92"
      },
      "source": [
        "print('Нормализованное распределение по классам в начальном датасете')\n",
        "print(dataframe.value_counts('label', normalize=True))\n",
        "\n",
        "print('Нормализованное распределение по классам в датасете train')\n",
        "print(train.value_counts('label', normalize=True))\n",
        "\n",
        "print('Нормализованное распределение по классам в датасете test')\n",
        "print(test.value_counts('label', normalize=True))\n",
        "\n",
        "print('Нормализованное распределение по классам в датасете val')\n",
        "print(val.value_counts('label', normalize=True))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Нормализованное распределение по классам в начальном датасете\n",
            "label\n",
            "3    0.759205\n",
            "1    0.127446\n",
            "0    0.107456\n",
            "2    0.005893\n",
            "dtype: float64\n",
            "Нормализованное распределение по классам в датасете train\n",
            "label\n",
            "3    0.760539\n",
            "1    0.125718\n",
            "0    0.107724\n",
            "2    0.006018\n",
            "dtype: float64\n",
            "Нормализованное распределение по классам в датасете test\n",
            "label\n",
            "3    0.760062\n",
            "1    0.128538\n",
            "0    0.105142\n",
            "2    0.006258\n",
            "dtype: float64\n",
            "Нормализованное распределение по классам в датасете val\n",
            "label\n",
            "3    0.752798\n",
            "1    0.132988\n",
            "0    0.109279\n",
            "2    0.004934\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Mw9DQ2duiP"
      },
      "source": [
        "Распределение классов одинаково во всех наборах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3tc82FsT-pN"
      },
      "source": [
        "## Выбор модели энкодера\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL8n0cV9aSW1"
      },
      "source": [
        "Для работы выбрана модель [LaBSE](https://tfhub.dev/google/LaBSE/2) (Language-agnostic BERT sentence embedding model) как одна из наиболее актуальных моделей, показывающих хорошие результаты для русского языка.\n",
        "\n",
        "Данные для этого энкодера должны быть предварительно обработаны препроцессором [universal-sentence-encoder-cmlm/multilingual-preprocess](https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV6Bg11IcWrm"
      },
      "source": [
        "def get_model(model_url, max_seq_length):\n",
        "  labse_layer = hub.KerasLayer(model_url, trainable=True)\n",
        "\n",
        "  # Define input.\n",
        "  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                         name=\"input_word_ids\")\n",
        "  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                     name=\"input_mask\")\n",
        "  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                      name=\"segment_ids\")\n",
        "\n",
        "  # LaBSE layer.\n",
        "  pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "  # The embedding is l2 normalized.\n",
        "  pooled_output = tf.keras.layers.Lambda(\n",
        "      lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n",
        "\n",
        "  # Define model.\n",
        "  return tf.keras.Model(\n",
        "        inputs=[input_word_ids, input_mask, segment_ids],\n",
        "        outputs=pooled_output), labse_layer\n",
        "\n",
        "\n",
        "labse_model, labse_layer = get_model(\n",
        "    model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=256)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBm2J6mvejtm"
      },
      "source": [
        "## Изменение стандартного препроцессора"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRfr7LdrevJ7"
      },
      "source": [
        "Стандартный препроцессор получает на вход текст и выдает в качестве output словарь из трех тензоров:  \n",
        "- `input_word_ids`: id поданных на вход слов  \n",
        "- `input_mask`: маска из 1 и 0, где 1 находится на позициях значимых слов, 0 на позициях паддинга  \n",
        "- `input_type_ids`: маска из 0 и 1 для передачи дополнительной информации о токенах\n",
        "\n",
        "Нам необходимо изменить препроцессор таким образом, что в маске `input_type_ids` на позициях токенов интересующего нас объекта стояли 1, на всех остальных позициях - 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdQ-goaboq0u",
        "outputId": "5ad1da38-2d61-4fe0-faed-8f8e94ee04df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |████████                        | 10 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 88 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.0)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30534 sha256=87174e32176b1f4a6beaaf97e8e8367c9924cb80da11aa85043ab2ce03cd5e34\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19473 sha256=9a8d86a3acee0bb09d023d2748e4ef3ba3f93c785030930e45ba54acd9fb6a4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7912 sha256=3185dc85279213e6aa0390befd9ab217ddc7b812076faa4f9fb9c9dda6bc36a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl3cPyENu83m",
        "outputId": "13775d75-bf8e-4030-d042-c4f80cfdaedd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYCQch-BfBtr"
      },
      "source": [
        "import bert\n",
        "\n",
        "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "def create_input(input_strings, entity_starts, entity_ends, tokenizer, max_seq_length):\n",
        "\n",
        "  input_ids_all, input_mask_all, segment_ids_all = [], [], []\n",
        "  for i in range (len(input_strings)):\n",
        "    #Tokenize input calculating entity start and end positions\n",
        "    input_string = input_strings[i]\n",
        "    entity_start = entity_starts[i]\n",
        "    entity_end = entity_ends[i]\n",
        "    input_tokens_before_entity = [\"[CLS]\"] + tokenizer.tokenize(input_string[:entity_start])\n",
        "    input_ids_before_entity = tokenizer.convert_tokens_to_ids(input_tokens_before_entity)\n",
        "    before_entity_length = len(input_ids_before_entity)\n",
        "    input_tokens_entity = tokenizer.tokenize(input_string[entity_start:entity_end])\n",
        "    input_ids_entity = tokenizer.convert_tokens_to_ids(input_tokens_entity)\n",
        "    entity_length = len(input_ids_entity)\n",
        "    input_tokens_after_entity = tokenizer.tokenize(input_string[entity_end:])\n",
        "    input_ids_after_entity = tokenizer.convert_tokens_to_ids(input_tokens_after_entity)\n",
        "    after_entity_length = len(input_ids_after_entity)\n",
        "\n",
        "    input_ids = input_ids_before_entity + input_ids_entity + input_ids_after_entity\n",
        "    sequence_length = before_entity_length+entity_length+after_entity_length\n",
        "\n",
        "    # Padding or truncation.\n",
        "    if len(input_ids) >= max_seq_length:\n",
        "      input_ids = input_ids[:max_seq_length]\n",
        "    else:\n",
        "      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
        "\n",
        "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
        "\n",
        "    segment_ids = [0] * before_entity_length + [1] * entity_length + [0] * after_entity_length\n",
        "    if(len(segment_ids)>max_seq_length):\n",
        "      segment_ids = segment_ids[:max_seq_length]\n",
        "    else:\n",
        "      segment_ids = segment_ids + [0]*(max_seq_length - len(segment_ids))\n",
        "\n",
        "    input_ids_all.append(input_ids)\n",
        "    input_mask_all.append(input_mask)\n",
        "    segment_ids_all.append(segment_ids)\n",
        "\n",
        "  return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)\n",
        "\n",
        "def encode(input_text, entity_start, entity_end):\n",
        "  input_ids, input_mask, segment_ids = create_input(\n",
        "    input_text, entity_start, entity_end, tokenizer, 256)\n",
        "  return labse_model([input_ids, input_mask, segment_ids])\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOdIdlRQwnq8",
        "outputId": "9143e36f-223d-47ae-ad43-9ee1d7b7b8e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_text = dataframe['text'].to_list()[:5]\n",
        "test_start = dataframe['entity_span_start'].to_list()[:5]\n",
        "test_end = dataframe['entity_span_end'].to_list()[:5]\n",
        "print(test_text)\n",
        "print(test_start)\n",
        "print(test_end)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\"да, я допустил неточность, а посол Мюррей ссылается на слухи... но зато теперь мы накопали много РЕАЛЬНОЙ информации на Усманова', '\"да, я допустил неточность, а посол Мюррей ссылается на слухи... но зато теперь мы накопали много РЕАЛЬНОЙ информации на Усманова', '\"Тот персонаж, о котором вы упомянули [Навальный]..., это тот человек, кого они[американская администрация] хотели бы продвинуть в политическую сферу России и видеть в руководстве страны\", заметил Путин, подчеркнув, что США \"в этом смысле прокололись', '\"Тот персонаж, о котором вы упомянули [Навальный]..., это тот человек, кого они[американская администрация] хотели бы продвинуть в политическую сферу России и видеть в руководстве страны\", заметил Путин, подчеркнув, что США \"в этом смысле прокололись', '\"Хотелось бы знать, Владимир Владимирович, сколько еще на своем посту пробудет губернатор Дарькин, который весь этот беспредел контролирует от начала и до конца и, мягко говоря, набивает карманы?\" В ответ Путин пообещал учитывать сказанное \"в будущей работе']\n",
            "[36, 121, 197, 39, 205]\n",
            "[42, 129, 202, 48, 210]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FizZvD0EesSN",
        "outputId": "a45754b6-2365-47b7-adb9-81165deb550f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(encode(test_text, test_start, test_end))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.00392828  0.01847563 -0.05777057 ...  0.04605664 -0.07479083\n",
            "  -0.05366782]\n",
            " [ 0.00221529  0.01619829 -0.0579958  ...  0.04617842 -0.07406548\n",
            "  -0.05412472]\n",
            " [-0.04256107  0.01346303 -0.01689823 ... -0.01053682 -0.05535734\n",
            "  -0.04274627]\n",
            " [-0.04247919  0.01359009 -0.01684185 ... -0.01040957 -0.05528823\n",
            "  -0.04277295]\n",
            " [-0.00665717 -0.04999565  0.04001128 ... -0.01140187  0.00798334\n",
            "  -0.05595917]], shape=(5, 768), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMwgJjP71Hc5"
      },
      "source": [
        "## Построение модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN0rZjAJxe2P"
      },
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = labse_preprocessor \n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = labse_encoder\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9EYxg1XpeKA",
        "outputId": "1d61b2d6-4bfd-47c5-fc9b-bf4c4cb6e400"
      },
      "source": [
        "classifier_model = build_classifier_model()\n",
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(tf.sigmoid(bert_raw_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.48670408]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhxFSL1ozegn"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "-lFXK0bLzfNz",
        "outputId": "5d45d0b3-b23b-405d-98f4-f70db352ee73"
      },
      "source": [
        "epochs = 5\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-ccf87087df35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_train_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
          ]
        }
      ]
    }
  ]
}