{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPZJ9ZOw+k46tmQUfic4if",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shatokua/sent_analysis_dataset/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mWCmZcM_KRi"
      },
      "source": [
        "# Анализ тональности по отношению к выбранному объекту"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iOUJmfS_auK"
      },
      "source": [
        "Конечной целью исследования является отладка модели для определения тональности текста по отношению к выбранному объекту текста, например, к упомянутой в тексте персоне, организации и т.д. \n",
        "\n",
        "Такой анализ является более точным, чем стандартный анализ тональности, и представляет больший интерес для конечных пользователей, поскольку дает оценку отношения к конкретному объекту, а не ко всему тексту."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK0Q-H5pAnfX"
      },
      "source": [
        "## Подготовка окружения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTTzDjapgr-p"
      },
      "source": [
        "%%capture\n",
        "#@title Установка окружения\n",
        "\n",
        "!pip install -q sklearn==0.22.2.post1\n",
        "!pip install mendelai-brat-parser==0.0.4\n",
        "!pip install smart_open==5.1.0\n",
        "!pip install tensorflow-text==2.5.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWVHhTq6gxJD"
      },
      "source": [
        "%%capture\n",
        "#@title Импорт библиотек\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from shutil import copyfile\n",
        "from brat_parser import get_entities_relations_attributes_groups\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text  # для загрузки universal-sentence-encoder-cmlm/multilingual-preprocess\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ROOsxkHnF8"
      },
      "source": [
        "%%capture\n",
        "#@title Определение рабочих директорий с данными\n",
        "SOURCE_DIR1 = 'done/'\n",
        "SOURCE_DIR2 = 'done1/'\n",
        "BASE_DIR = 'train_test/'\n",
        "main_csv_file = 'sent_quotes_done.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MX2mnYvA8KV"
      },
      "source": [
        "## Изучение и подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZDe_LwrBQGE"
      },
      "source": [
        "Данные представлены в виде текстовых файлов и файлов аннотаций в формате BRAT.\n",
        "Все файлы находятся в двух папках done и done1. Файлы внутри папок не разделены по классам. Если в одном файле упоминается несколько объектов, по отношению к которым определяется тональность, данные находятся в одном файле ann.\n",
        "\n",
        "Для работы в Tensorflow датасет может быть представлен в двух форматах:\n",
        "1) как структурированный файл (например, csv), где каждая колонка является либо признаком, либо меткой класса;\n",
        "2) набор файлов, распределенных по директориям-классам. \n",
        "\n",
        "Кроме того, необходимо создать копии\n",
        "\n",
        "Подготовим данные для работы с Tensorflow\n",
        "(**для последующей работы с Google Colab в дальнейшем будет использоваться сохраненный файл csv, исполнение следующих двух ячеек не требуется**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-vixGELH5lN"
      },
      "source": [
        "%%capture\n",
        "#@title Функция для перевода данных в читаемый TensorFlow формат\n",
        "\n",
        "def files_to_df(source_dir, target_dir):\n",
        "    \"\"\"Собираем список файлов txt. \n",
        "    Идём по списку файлов, для каждого файла извлекаем текст сообщения (из txt), \n",
        "    текст объекта (ann), его индексы (ann), тональность (ann). \n",
        "    Раскладываем файлы на папки по классам, параллельно записываем в \n",
        "    dataframe pandas\"\"\"\n",
        "\n",
        "    if (target_dir[:-1] not in os.listdir()):\n",
        "      os.mkdir(target_dir)\n",
        "\n",
        "    file_names = [fn[:-4] for fn in os.listdir(source_dir) if fn[-3:]=='txt']\n",
        "    \n",
        "    text_to_process = []\n",
        "    \n",
        "    for fn in file_names:\n",
        "        txt_file_path = source_dir + fn +'.txt'\n",
        "        \n",
        "        with open(txt_file_path, encoding=\"utf8\") as f:\n",
        "            txt = f.read()\n",
        "        \n",
        "        ann_file_path = source_dir + fn +'.ann'\n",
        "        entities, relations, attributes, groups = get_entities_relations_attributes_groups(ann_file_path)\n",
        "        entities_keys = list(entities.keys())\n",
        "        \n",
        "        for key in entities_keys:\n",
        "            class_dir = entities[key].type\n",
        "            if (class_dir not in os.listdir(target_dir)):\n",
        "                os.mkdir(target_dir + class_dir)\n",
        "                \n",
        "            entity_id = entities[key].id\n",
        "\n",
        "            text_to_process.append({'filename':fn+'_'+entity_id, 'text': txt, 'entity_id': entities[key].id, \n",
        "                                    'entity_text': entities[key].text, 'entity_span': entities[key].span[0],\n",
        "                                   'label': entities[key].type})\n",
        "            \n",
        "            txt_new_file_path = target_dir + class_dir + '/' + fn + '_' + entity_id +'.txt'\n",
        "            copyfile(txt_file_path, txt_new_file_path)\n",
        "            ent_new_file_path = target_dir + class_dir + '/' + fn + '_' + entity_id + '_entity' +'.txt'\n",
        "            with open(ent_new_file_path, mode ='w', encoding =\"utf8\") as f:\n",
        "              f.write(entities[key].text)\n",
        "    \n",
        "    df = pd.DataFrame.from_dict(text_to_process)\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eIghZpYJ-OX"
      },
      "source": [
        "%%capture\n",
        "#@title Изменение структуры папок, перенос данных в csv\n",
        "\n",
        "df1 = files_to_df(SOURCE_DIR1, BASE_DIR)\n",
        "df2 = files_to_df(SOURCE_DIR2, BASE_DIR)\n",
        "df = pd.concat([df1, df2])\n",
        "df.to_csv(main_csv_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcsmPtqlLaBY"
      },
      "source": [
        "Для работы в Google Colab требуется загрузка csv-файла."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhMVz0OVgzPa"
      },
      "source": [
        "%%capture\n",
        "#@title Чтение файл csv\n",
        "\n",
        "csv_file = main_csv_file\n",
        "dataframe = pd.read_csv(csv_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylGTDnSSOdb-"
      },
      "source": [
        "Изучим распределение датасета по классам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEgvHhZQ-cHF"
      },
      "source": [
        "%%capture\n",
        "#@title Распределение классов\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjLWDdHN-tQV",
        "outputId": "44354720-da1c-4dc3-d3f4-c09c6078923d"
      },
      "source": [
        "print(dataframe.value_counts('label'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label\n",
            "Neutral_all     29070\n",
            "Neutral         10354\n",
            "Negative         4836\n",
            "Positive         4657\n",
            "Negative_all     1782\n",
            "Positive_all      923\n",
            "Mixed             291\n",
            "Mixed_all          15\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOAsFHEAWdKk"
      },
      "source": [
        "Классы несбалансированы. Класс Mixed_all состоит всего из 15 экземпляров. Укрупним классы, объединив классы с постфиксом _all с одноименными без префикса.\n",
        "Также приведем метки классов к числовым категориям"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDrs20INhOwg"
      },
      "source": [
        "%%capture\n",
        "#@title Объединение классов, приведение меток классов к категориальному формату\n",
        "\n",
        "classes = {'Positive': 0, 'Positive_all': 0, 'Negative': 1, 'Negative_all': 1,  \n",
        "           'Mixed': 2, 'Mixed_all': 2, 'Neutral': 3, 'Neutral_all': 3}\n",
        "dataframe['label'] = dataframe['label'].apply(lambda x: classes[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSOCZz0PXfv7"
      },
      "source": [
        "Разделим датасет на набор данных для тренировки и для тестирования.\n",
        "В тренировочном датасете выделим набор для валидации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgvSFWCklqE5"
      },
      "source": [
        "%%capture\n",
        "#@title Объединение классов, приведение меток классов к категориальному формату\n",
        "\n",
        "train, test = train_test_split(dataframe, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzmsqxAP-1EX",
        "outputId": "3e2080c8-edbb-4efd-b587-23acc03e656a"
      },
      "source": [
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33233 train examples\n",
            "8309 validation examples\n",
            "10386 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOIMMS-BX4S0"
      },
      "source": [
        "Поскольку классы не сбалансированы, проверим распределение по классам внутри тренировочного, тестового и валидационного датасетов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mkNVEu9YQ29"
      },
      "source": [
        "%%capture\n",
        "#@title Проверка распределения классов\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uo2fQAH-6M9",
        "outputId": "404ea432-616e-4bda-8fde-b2edad1f4fd6"
      },
      "source": [
        "print('Нормализованное распределение по классам в начальном датасете')\n",
        "print(dataframe.value_counts('label', normalize=True))\n",
        "\n",
        "print('Нормализованное распределение по классам в датасете train')\n",
        "print(train.value_counts('label', normalize=True))\n",
        "\n",
        "print('Нормализованное распределение по классам в датасете test')\n",
        "print(test.value_counts('label', normalize=True))\n",
        "\n",
        "print('Нормализованное распределение по классам в датасете val')\n",
        "print(val.value_counts('label', normalize=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Нормализованное распределение по классам в начальном датасете\n",
            "label\n",
            "3    0.759205\n",
            "1    0.127446\n",
            "0    0.107456\n",
            "2    0.005893\n",
            "dtype: float64\n",
            "Нормализованное распределение по классам в датасете train\n",
            "label\n",
            "3    0.757049\n",
            "1    0.127644\n",
            "0    0.109439\n",
            "2    0.005868\n",
            "dtype: float64\n",
            "Нормализованное распределение по классам в датасете test\n",
            "label\n",
            "3    0.764106\n",
            "1    0.126805\n",
            "0    0.103216\n",
            "2    0.005873\n",
            "dtype: float64\n",
            "Нормализованное распределение по классам в датасете val\n",
            "label\n",
            "3    0.761704\n",
            "1    0.127452\n",
            "0    0.104826\n",
            "2    0.006018\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Mw9DQ2duiP"
      },
      "source": [
        "Распределение классов одинаково во всех наборах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3tc82FsT-pN"
      },
      "source": [
        "## Выбор модели энкодера\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL8n0cV9aSW1"
      },
      "source": [
        "Для работы выбрана модель [LaBSE](https://tfhub.dev/google/LaBSE/2) (Language-agnostic BERT sentence embedding model) как одна из наиболее актуальных моделей, показывающих хорошие результаты для русского языка.\n",
        "\n",
        "Данные для этого энкодера должны быть предварительно обработаны препроцессором [universal-sentence-encoder-cmlm/multilingual-preprocess](https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV6Bg11IcWrm"
      },
      "source": [
        "labse_preprocessor = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
        "labse_encoder = hub.KerasLayer(\"https://tfhub.dev/google/LaBSE/2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBm2J6mvejtm"
      },
      "source": [
        "## Изменение стандартного препроцессора"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRfr7LdrevJ7"
      },
      "source": [
        "Стандартный препроцессор получает на вход текст и выдает в качестве output словарь из трех тензоров:  \n",
        "- `input_word_ids`: id поданных на вход слов  \n",
        "- `input_mask`: маска из 1 и 0, где 1 находится на позициях значимых слов, 0 на позициях паддинга  \n",
        "- `input_type_ids`: маска из 0 и 1 для передачи дополнительной информации о токенах\n",
        "\n",
        "Нам необходимо изменить препроцессор таким образом, что в маске `input_type_ids` на позициях токенов интересующего нас объекта стояли 1, на всех остальных позициях - 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYCQch-BfBtr"
      },
      "source": [
        "%%capture\n",
        "#@title Вспомогательные функции \n",
        "\n",
        "\"\"\"\n",
        "Вспомогательные функции для поиска в NumPy-массиве input_word_ids, \n",
        "соответствующих input_word_ids объекта\n",
        "\"\"\"\n",
        "\n",
        "def rolling_window(a, window):\n",
        "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
        "    strides = a.strides + (a.strides[-1],)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "           \n",
        "def findFirst_numpy(a, b):\n",
        "    temp = rolling_window(a, len(b))\n",
        "    result = np.where(np.all(temp == b, axis=1))\n",
        "    return result[0][0] if result else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FizZvD0EesSN"
      },
      "source": [
        "%%capture\n",
        "#@title Функция для изменения массива type_id\n",
        "\n",
        "\n",
        "def change_typeid(text_preprocessed, token_text, preprocessor): \n",
        "  \"\"\"\n",
        "  Принимает на вход результаты обработки строки препроцессором BERT\n",
        "  (словарь с ключами input_word_ids, input_mask, input_type_ids)\n",
        "  и текст токена.\n",
        "  Меняет значения input_type_ids таким образом, что на позициях, соответствующих\n",
        "  строке объекта token_text проставляются 1 вместо 0\n",
        "  Возвращает маску из нолей и единиц для input_type_ids\n",
        "  \"\"\"\n",
        "  token_preprocessed = preprocessor(token_text)\n",
        "  token_numpy = token_preprocessed['input_word_ids'].numpy()[0]\n",
        "  start = 1\n",
        "  end = np.where(token_numpy == 102)[0][0]\n",
        "  token_codes = token_numpy[start:end]\n",
        "\n",
        "  text_numpy = text_preprocessed['input_word_ids'].numpy()[0]\n",
        "\n",
        "  indx_start = findFirst_numpy(text_numpy, token_codes)\n",
        "  indx_end = indx_start+len(token_codes)\n",
        "\n",
        "  pattern_to_modify = text_preprocessed[\"input_type_ids\"][0].numpy()\n",
        "  pattern_to_modify[indx_start: indx_end] = 1\n",
        "  pattern_to_modify = [pattern_to_modify]\n",
        "\n",
        "  text_preprocessed[\"input_type_ids\"] = tf.add (text_preprocessed[\"input_type_ids\"], pattern_to_modify)\n",
        "  return pattern_to_modify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB2v86P1c6Ks"
      },
      "source": [
        "def custom_preprocess(texts, entities, preprocessor):\n",
        "  inputs = preprocessor(texts)\n",
        "  print(inputs)\n",
        "  for i in range(inputs.shape[0]):\n",
        "    change_typeid(inputs[i], entities[i], preprocessor)\n",
        "  return inputs    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73RdoDBfhEZd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e427e1d-20e6-4f3f-d112-603ec328dd3f"
      },
      "source": [
        "text_test = ['\"да, я допустил неточность, а посол Мюррей ссылается на слухи... но зато теперь мы накопали много РЕАЛЬНОЙ информации на Усманова', 'Мы тогда с сыном даже пять месяцев потом жили у Аллы Пугачевой. Порой она ночью садилась за рояль, а я ей говорил: \"Тише. Максима разбудишь\". Я все время искал с Алисой встречи. Помню, перелезал через забор в каком-то санатории. Регина делал все, чтобы мы отдалились. Это такой женский эгоизм» ']\n",
        "token_text = ['Мюррей', 'Аллы Пугачевой']\n",
        "text_preprocessed = custom_preprocess(text_test, token_text, labse_preprocessor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_type_ids': <tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "      dtype=int32)>, 'input_word_ids': <tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
            "array([[   101,    107,  15018,    117,    728, 213346, 348977,  15573,\n",
            "        467560, 183099,    117,    697, 478019,    677, 162813, 108855,\n",
            "           714,  16772,  15950,  32409,  14984, 399760,    119,    119,\n",
            "           119,  15635,  66052,  33972,  16715,  14984, 213508,  27036,\n",
            "         16975,    681,  31759, 323575, 115722, 121401,  87184,  25314,\n",
            "         14984, 284809, 200439,  15097,    102,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0],\n",
            "       [   101,  18658,  34100,    714, 360900,  19578,  61943,  45140,\n",
            "         31369, 152531,    716,  18856,  17875,    680,  77545, 380797,\n",
            "           119,  17420,  77626,  18462, 129814,  48240, 159383,  15029,\n",
            "         31052,  85203,  16428,    117,    697,    728,  39367, 121737,\n",
            "           131,    107,  46512,  18138,    119, 369540,  18746, 347373,\n",
            "         45513,    107,    119,    696,  15983,  17255, 190998,    714,\n",
            "         39595,  76646,  15307,  98994,    119,  17420, 312214,  15846,\n",
            "           117, 304994,  77410, 119691,  17901, 470052,    699, 109779,\n",
            "           118,  15494, 218829, 406028,    119,    681, 436571,  15097,\n",
            "        245814,  15983,    117,  17339,  16715,  15146,  57787,  27575,\n",
            "           119,  18586,  24140, 347811, 436847,  83482,    226,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0]],\n",
            "      dtype=int32)>, 'input_mask': <tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "      dtype=int32)>}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-151ddefaf1ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'\"да, я допустил неточность, а посол Мюррей ссылается на слухи... но зато теперь мы накопали много РЕАЛЬНОЙ информации на Усманова'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Мы тогда с сыном даже пять месяцев потом жили у Аллы Пугачевой. Порой она ночью садилась за рояль, а я ей говорил: \"Тише. Максима разбудишь\". Я все время искал с Алисой встречи. Помню, перелезал через забор в каком-то санатории. Регина делал все, чтобы мы отдалились. Это такой женский эгоизм» '\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtoken_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Мюррей'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Аллы Пугачевой'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtext_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabse_preprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-3c75e88c16cc>\u001b[0m in \u001b[0;36mcustom_preprocess\u001b[0;34m(texts, entities, preprocessor)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mchange_typeid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdbMuW-ZX8-Y",
        "outputId": "c34a17f5-05a4-4d88-d4f3-fb3fccd33d98"
      },
      "source": [
        "text_preprocessed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[   101,    107,  15018,    117,    728, 213346, 348977,  15573,\n",
              "         467560, 183099,    117,    697, 478019,    677, 162813, 108855,\n",
              "            714,  16772,  15950,  32409,  14984, 399760,    119,    119,\n",
              "            119,  15635,  66052,  33972,  16715,  14984, 213508,  27036,\n",
              "          16975,    681,  31759, 323575, 115722, 121401,  87184,  25314,\n",
              "          14984, 284809, 200439,  15097,    102,      0,      0,      0,\n",
              "              0,      0,      0,      0,      0,      0,      0,      0,\n",
              "              0,      0,      0,      0,      0,      0,      0,      0,\n",
              "              0,      0,      0,      0,      0,      0,      0,      0,\n",
              "              0,      0,      0,      0,      0,      0,      0,      0,\n",
              "              0,      0,      0,      0,      0,      0,      0,      0,\n",
              "              0,      0,      0,      0,      0,      0,      0,      0,\n",
              "              0,      0,      0,      0,      0,      0,      0,      0,\n",
              "              0,      0,      0,      0,      0,      0,      0,      0,\n",
              "              0,      0,      0,      0,      0,      0,      0,      0,\n",
              "              0,      0,      0,      0,      0,      0,      0,      0]],\n",
              "       dtype=int32)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57YquaLtVmxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e560bb5d-ce20-4f22-982a-1e76be98708c"
      },
      "source": [
        "text_test = ['\"да, я допустил неточность, а посол Мюррей ссылается на слухи... но зато теперь мы накопали много РЕАЛЬНОЙ информации на Усманова', 'Мы тогда с сыном даже пять месяцев потом жили у Аллы Пугачевой. Порой она ночью садилась за рояль, а я ей говорил: \"Тише. Максима разбудишь\". Я все время искал с Алисой встречи. Помню, перелезал через забор в каком-то санатории. Регина делал все, чтобы мы отдалились. Это такой женский эгоизм» ']\n",
        "token_text = ['Мюррей', 'Аллы Пугачевой']\n",
        "text_preprocessed = labse_preprocessor(text_test)\n",
        "token_preprocessed = labse_preprocessor(token_text)\n",
        "print(text_preprocessed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_type_ids': <tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "      dtype=int32)>, 'input_word_ids': <tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
            "array([[   101,    107,  15018,    117,    728, 213346, 348977,  15573,\n",
            "        467560, 183099,    117,    697, 478019,    677, 162813, 108855,\n",
            "           714,  16772,  15950,  32409,  14984, 399760,    119,    119,\n",
            "           119,  15635,  66052,  33972,  16715,  14984, 213508,  27036,\n",
            "         16975,    681,  31759, 323575, 115722, 121401,  87184,  25314,\n",
            "         14984, 284809, 200439,  15097,    102,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0],\n",
            "       [   101,  18658,  34100,    714, 360900,  19578,  61943,  45140,\n",
            "         31369, 152531,    716,  18856,  17875,    680,  77545, 380797,\n",
            "           119,  17420,  77626,  18462, 129814,  48240, 159383,  15029,\n",
            "         31052,  85203,  16428,    117,    697,    728,  39367, 121737,\n",
            "           131,    107,  46512,  18138,    119, 369540,  18746, 347373,\n",
            "         45513,    107,    119,    696,  15983,  17255, 190998,    714,\n",
            "         39595,  76646,  15307,  98994,    119,  17420, 312214,  15846,\n",
            "           117, 304994,  77410, 119691,  17901, 470052,    699, 109779,\n",
            "           118,  15494, 218829, 406028,    119,    681, 436571,  15097,\n",
            "        245814,  15983,    117,  17339,  16715,  15146,  57787,  27575,\n",
            "           119,  18586,  24140, 347811, 436847,  83482,    226,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0]],\n",
            "      dtype=int32)>, 'input_mask': <tf.Tensor: shape=(2, 128), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "      dtype=int32)>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlcSOOyTuNaJ"
      },
      "source": [
        "def rolling_window(a, window):\n",
        "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
        "    strides = a.strides + (a.strides[-1],)\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
        "           \n",
        "def findFirst_numpy(a, b):\n",
        "    temp = rolling_window(a, len(b))\n",
        "    result = np.where(np.all(temp == b, axis=1))\n",
        "    return result[0][0] if result else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxM3LVCOWwLp"
      },
      "source": [
        "def change_typeid_arr(text_preprocessed, token_text, preprocessor):\n",
        "  token_preprocessed = preprocessor(token_text)\n",
        "\n",
        "  token_numpy = token_preprocessed['input_word_ids'].numpy()\n",
        "  print (token_numpy)\n",
        "\n",
        "  start = 1\n",
        "  end = np.where(token_numpy == 102)\n",
        "  print (end)\n",
        "\n",
        "\n",
        "  text_numpy = text_preprocessed['input_word_ids'].numpy()\n",
        "\n",
        "\n",
        "#  patterns = []\n",
        "#  for j in range(len(text_numpy)):\n",
        "#    indx_start = findFirst_numpy(text_numpy[j], token_codes[j])\n",
        "#    indx_end = indx_start+len(token_codes[j])\n",
        "\n",
        "#    pattern_to_modify = text_preprocessed[\"input_type_ids\"][j].numpy()\n",
        "#    pattern_to_modify[indx_start: indx_end] = 1\n",
        "#    patterns.append(pattern_to_modify)\n",
        "#  patterns = np.asarray(patterns)\n",
        "\n",
        "#  text_preprocessed[\"input_type_ids\"] = tf.add (text_preprocessed[\"input_type_ids\"], patterns)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_Ap4ilLq8M2",
        "outputId": "aa99afb1-ea47-42f0-9246-7a9af7a184a5"
      },
      "source": [
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type Ids   : [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUKVKajy2-4G",
        "outputId": "57845848-5141-4d14-b7f7-92cc6508ae88"
      },
      "source": [
        "change_typeid_arr(text_preprocessed, token_text, labse_preprocessor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101    677 162813 108855    102      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0]\n",
            " [   101  18856  17875    680  77545 380797    102      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0]]\n",
            "(array([0, 1]), array([4, 6]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGokwNrIjs-5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm6Rf2yPlrxw",
        "outputId": "334d4c25-6508-4cd6-a3c8-544871cf828f"
      },
      "source": [
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type Ids   : [[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHyahYdsnCuB"
      },
      "source": [
        "bert_results = encoder(text_preprocessed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNXTRHCadzVP"
      },
      "source": [
        "## Преобразование данных в датасет Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2TUXW0biw7G"
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(train.label, num_classes=4)\n",
        "y_val = tf.keras.utils.to_categorical(val.label, num_classes=4)\n",
        "y_test = tf.keras.utils.to_categorical(test.label, num_classes=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXbwRbTz_WAV"
      },
      "source": [
        "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"Генерирует батчи данных\n",
        "\n",
        "    Аргументы:\n",
        "        texts_and_entities: массив текстов и объектов\n",
        "        labels: массив меток\n",
        "        batch_size: размер батча (int)\n",
        "        shuffle: bool, перемешивать ли данные\n",
        "        include_targets: bool, включать ли метки\n",
        "\n",
        "    Возвращает:\n",
        "        Tuple `([input_ids, attention_mask, `token_type_ids], labels)`\n",
        "        (или только `[input_ids, attention_mask, `token_type_ids]`\n",
        "         если `include_targets=False`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts_and_entities,\n",
        "        labels,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        include_targets=True,\n",
        "    ):\n",
        "        self.texts_and_entities = texts_and_entities\n",
        "        self.labels = labels\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.include_targets = include_targets\n",
        "        \n",
        "        # Загружаем токенизатор BERT\n",
        "        self.tokenizer =  hub.KerasLayer(preprocessor.tokenize)\n",
        "        self.indexes = np.arange(len(self.texts_and_entities))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Denotes the number of batches per epoch.\n",
        "        return len(self.sentence_pairs) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieves the batch of index.\n",
        "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        texts_and_entities = self.texts_and_entities[indexes]\n",
        "\n",
        "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
        "        # encoded together and separated by [SEP] token.\n",
        "        encoded = self.tokenizer.batch_encode_plus(\n",
        "            sentence_pairs.tolist(),\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"tf\",\n",
        "        )\n",
        "\n",
        "        # Convert batch of encoded features to numpy array.\n",
        "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "        # Set to true if data generator is used for training/validation.\n",
        "        if self.include_targets:\n",
        "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
        "            return [input_ids, attention_masks, token_type_ids], labels\n",
        "        else:\n",
        "            return [input_ids, attention_masks, token_type_ids]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
        "        if self.shuffle:\n",
        "            np.random.RandomState(42).shuffle(self.indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8oADJZ0zzu9",
        "outputId": "2d4f88ab-1a84-4d27-82e7-f895a3d629f2"
      },
      "source": [
        "## В утиль\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('type')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(batch_size)\n",
        "  return ds\n",
        "\n",
        "batch_size = 5\n",
        "train_ds = df_to_dataset(train[['text', 'entity_text','type']], batch_size=batch_size)\n",
        "\n",
        "[(train_features, label_batch)] = train_ds.take(1)\n",
        "print('Every feature:', list(train_features.keys()))\n",
        "print('A batch of targets:', label_batch )\n",
        "print('A batch of features (text):',train_features['text'])\n",
        "print('A batch of features (entity_text):',train_features['entity_text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Every feature: ['text', 'entity_text']\n",
            "A batch of targets: tf.Tensor([3 0 3 3 3], shape=(5,), dtype=int64)\n",
            "A batch of features (text): tf.Tensor(\n",
            "[b'\\xd0\\x9d\\xd0\\xbe \\xd1\\x82\\xd0\\xbe\\xd0\\xb3\\xd0\\xb4\\xd0\\xb0, \\xd0\\x9d\\xd0\\xb8\\xd0\\xba\\xd0\\xb8\\xd1\\x82\\xd0\\xb0 \\xd0\\xa1\\xd0\\xb5\\xd1\\x80\\xd0\\xb3\\xd0\\xb5\\xd0\\xb5\\xd0\\xb2\\xd0\\xb8\\xd1\\x87, \\xd1\\x8f \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb7\\xd1\\x8b\\xd0\\xb2\\xd0\\xb0\\xd1\\x8e \\xd0\\xb2\\xd0\\xb0\\xd1\\x81 \\xd0\\xb1\\xd1\\x8b\\xd1\\x82\\xd1\\x8c \\xd0\\xbf\\xd0\\xbe\\xd1\\x81\\xd0\\xbb\\xd0\\xb5\\xd0\\xb4\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd1\\x82\\xd0\\xb5\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd1\\x8b\\xd0\\xbc. \\xd0\\x92\\xd0\\xb5\\xd1\\x80\\xd0\\xbd\\xd0\\xb8\\xd1\\x82\\xd0\\xb5 \\xd0\\xb2\\xd0\\xb0\\xd1\\x88\\xd0\\xb5\\xd0\\xb3\\xd0\\xbe \\xd0\\x9e\\xd1\\x81\\xd0\\xba\\xd0\\xb0\\xd1\\x80\\xd0\\xb0 \\xd0\\xbc\\xd0\\xb5\\xd1\\x80\\xd0\\xb7\\xd0\\xba\\xd0\\xb8\\xd0\\xbc \\xd0\\xb0\\xd0\\xbc\\xd0\\xb5\\xd1\\x80\\xd0\\xb8\\xd0\\xba\\xd0\\xb0\\xd0\\xbd\\xd1\\x86\\xd0\\xb0\\xd0\\xbc, \\xd0\\xba\\xd0\\xbe\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd1\\x8b\\xd0\\xb5 \"\\xd0\\xb1\\xd0\\xbe\\xd0\\xbc\\xd0\\xb1\\xd1\\x8f\\xd1\\x82 \\xd0\\x98\\xd1\\x80\\xd0\\xb0\\xd0\\xba \\xd0\\xb8 \\xd0\\x9b\\xd0\\xb8\\xd0\\xb2\\xd0\\xb8\\xd1\\x8e\" \\xd0\\xb8 \"\\xd1\\x82\\xd0\\xb5\\xd1\\x80\\xd0\\xb7\\xd0\\xb0\\xd1\\x8e\\xd1\\x82\" \\xd1\\x80\\xd1\\x83\\xd0\\xba\\xd0\\xbe\\xd0\\xb2\\xd0\\xbe\\xd0\\xb4\\xd0\\xb8\\xd1\\x82\\xd0\\xb5\\xd0\\xbb\\xd0\\xb5\\xd0\\xb9 \\xd0\\xb4\\xd1\\x80\\xd1\\x83\\xd0\\xb3\\xd0\\xb8\\xd1\\x85 \\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd0\\xb0\\xd0\\xbd! \\xd0\\x92\\xd1\\x8b \\xd0\\xb6\\xd0\\xb5 \\xd0\\xb8\\xd1\\x85 \\xd0\\xbd\\xd0\\xb5 \\xd1\\x83\\xd0\\xb2\\xd0\\xb0\\xd0\\xb6\\xd0\\xb0\\xd0\\xb5\\xd1\\x82\\xd0\\xb5? \\xd0\\x90 \\xd0\\xb7\\xd0\\xb0\\xd0\\xbe\\xd0\\xb4\\xd0\\xbd\\xd0\\xbe \\xd0\\xb8 \"\\xd0\\xb3\\xd0\\xb5\\xd0\\xb9\\xd1\\x80\\xd0\\xbe\\xd0\\xbf\\xd0\\xb5\" \\xd0\\xb2\\xd0\\xb5\\xd1\\x80\\xd0\\xbd\\xd0\\xb8\\xd1\\x82\\xd0\\xb5 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb7\\xd1\\x8b \\xd0\\xb8\\xd0\\xb7 \\xd0\\x98\\xd1\\x81\\xd0\\xbf\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd0\\xb8, \\xd0\\xa4\\xd1\\x80\\xd0\\xb0\\xd0\\xbd\\xd1\\x86\\xd0\\xb8\\xd0\\xb8, \\xd0\\x98\\xd1\\x82\\xd0\\xb0\\xd0\\xbb\\xd0\\xb8\\xd0\\xb8 \\xd0\\xb2 \\xd0\\xbe\\xd1\\x82\\xd0\\xb2\\xd0\\xb5\\xd1\\x82 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xb8\\xd1\\x85 \\xd1\\x81\\xd0\\xb0\\xd0\\xbd\\xd0\\xba\\xd1\\x86\\xd0\\xb8\\xd0\\xb8'\n",
            " b'\\xd0\\x9e\\xd1\\x88\\xd0\\xb8\\xd0\\xb1\\xd0\\xba\\xd0\\xb0 \\xd0\\x94\\xd0\\xb5 \\xd0\\xa5\\xd0\\xb5\\xd0\\xb0 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8 \\xd0\\xb2\\xd1\\x82\\xd0\\xbe\\xd1\\x80\\xd0\\xbe\\xd0\\xbc \\xd0\\xb3\\xd0\\xbe\\xd0\\xbb\\xd0\\xb5? \\xd0\\x92\\xd1\\x81\\xd0\\xb5 \\xd1\\x81\\xd1\\x82\\xd0\\xb0\\xd0\\xbb\\xd0\\xba\\xd0\\xb8\\xd0\\xb2\\xd0\\xb0\\xd1\\x8e\\xd1\\x82\\xd1\\x81\\xd1\\x8f \\xd1\\x81 \\xd1\\x8d\\xd1\\x82\\xd0\\xb8\\xd0\\xbc. \\xd0\\xaf \\xd1\\x83\\xd0\\xb2\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xbd, \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd0\\xb2 \\xd1\\x81\\xd0\\xb0\\xd0\\xbc\\xd1\\x8b\\xd0\\xb5 \\xd0\\xb2\\xd0\\xb0\\xd0\\xb6\\xd0\\xbd\\xd1\\x8b\\xd0\\xb5 \\xd0\\xbc\\xd0\\xbe\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd1\\x82\\xd1\\x8b \\xd0\\xbe\\xd0\\xbd \\xd0\\xb1\\xd1\\x83\\xd0\\xb4\\xd0\\xb5\\xd1\\x82 \\xd1\\x82\\xd0\\xb0\\xd0\\xba\\xd0\\xb8\\xd0\\xbc, \\xd0\\xba\\xd0\\xb0\\xd0\\xba\\xd0\\xb8\\xd0\\xbc \\xd0\\xbc\\xd1\\x8b \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xb2\\xd1\\x8b\\xd0\\xba\\xd0\\xbb\\xd0\\xb8 \\xd0\\xb2\\xd1\\x81\\xd0\\xb5\\xd0\\xb3\\xd0\\xb4\\xd0\\xb0 \\xd0\\xb5\\xd0\\xb3\\xd0\\xbe \\xd0\\xb2\\xd0\\xb8\\xd0\\xb4\\xd0\\xb5\\xd1\\x82\\xd1\\x8c'\n",
            " b'\\xd0\\x98\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd1\\x87\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd0\\xbe \\xd0\\xb7\\xd0\\xb0 \\xd0\\xb1\\xd0\\xbe\\xd0\\xb9 \\xd0\\xa2\\xd0\\xb8\\xd1\\x82\\xd0\\xbe\\xd0\\xb2 \\xd0\\xb4\\xd0\\xbe\\xd0\\xb3\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd1\\x80\\xd0\\xb8\\xd0\\xb2\\xd0\\xb0\\xd0\\xbb\\xd1\\x81\\xd1\\x8f. \\xd0\\x9f\\xd0\\xbe\\xd1\\x82\\xd0\\xbe\\xd0\\xbc \\xd0\\x9a\\xd0\\xb8\\xd1\\x80\\xd0\\xb8\\xd0\\xbb\\xd0\\xbb \\xd0\\xa9\\xd0\\xb5\\xd0\\xba\\xd1\\x83\\xd1\\x82\\xd1\\x8c\\xd0\\xb5\\xd0\\xb2 (\\xd0\\xbe\\xd1\\x82 \\xd0\\xa4\\xd0\\x91\\xd0\\xa0 \\xe2\\x80\\x94 \\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xbc. \\xd1\\x80\\xd0\\xb5\\xd0\\xb4.) \\xd0\\xbe\\xd0\\xb1\\xd0\\xb5\\xd1\\x89\\xd0\\xb0\\xd0\\xbb \\xd0\\xb4\\xd0\\xb5\\xd0\\xbd\\xd1\\x8c\\xd0\\xb3\\xd0\\xb8 \\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb2\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb8, \\xd0\\xb8 \\xd0\\xb2\\xd1\\x81\\xd0\\xb5 \\xd1\\x80\\xd0\\xb5\\xd0\\xba\\xd0\\xb2\\xd0\\xb8\\xd0\\xb7\\xd0\\xb8\\xd1\\x82\\xd1\\x8b \\xd0\\xba\\xd0\\xb0\\xd1\\x80\\xd1\\x82\\xd1\\x8b \\xd1\\x8f \\xd0\\xb5\\xd0\\xbc\\xd1\\x83 \\xd0\\xbe\\xd1\\x82\\xd0\\xbf\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xbb\\xd1\\x8f\\xd0\\xbb. \\xd0\\x98\\xd0\\xbc\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd0\\xbe \\xd0\\x9a\\xd0\\xb8\\xd1\\x80\\xd0\\xb8\\xd0\\xbb\\xd0\\xbb \\xd0\\xb4\\xd0\\xbe\\xd0\\xbb\\xd0\\xb6\\xd0\\xb5\\xd0\\xbd \\xd0\\xb1\\xd1\\x8b\\xd0\\xbb \\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb2\\xd0\\xb5\\xd1\\x81\\xd1\\x82\\xd0\\xb8 \\xd0\\xb4\\xd0\\xb5\\xd0\\xbd\\xd1\\x8c\\xd0\\xb3\\xd0\\xb8\\xc2\\xbb '\n",
            " b'\\xd0\\x9c\\xd0\\xb0\\xd1\\x88\\xd0\\xb8\\xd0\\xbd\\xd0\\xb0 \\xd0\\xbf\\xd0\\xb5\\xd0\\xb2\\xd0\\xb8\\xd1\\x86\\xd1\\x8b \\xd0\\x90\\xd0\\xbb\\xd0\\xbb\\xd1\\x8b \\xd0\\x9f\\xd1\\x83\\xd0\\xb3\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd0\\xb2\\xd0\\xbe\\xd0\\xb9 \\xd0\\xb4\\xd0\\xb2\\xd0\\xb8\\xd0\\xb3\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0\\xd1\\x81\\xd1\\x8c \\xd0\\xbf\\xd0\\xbe \\xd1\\x82\\xd0\\xb5\\xd1\\x85\\xd0\\xbd\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd0\\xb9 \\xd0\\xbf\\xd0\\xbb\\xd0\\xb0\\xd1\\x82\\xd1\\x84\\xd0\\xbe\\xd1\\x80\\xd0\\xbc\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xa0\\xd0\\xb8\\xd0\\xb6\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd0\\xbc \\xd0\\xb2\\xd0\\xbe\\xd0\\xba\\xd0\\xb7\\xd0\\xb0\\xd0\\xbb\\xd0\\xb5 \\xd0\\xb2 \\xd0\\x9c\\xd0\\xbe\\xd1\\x81\\xd0\\xba\\xd0\\xb2\\xd0\\xb5 \\xd0\\xb8, \\xd0\\xbf\\xd0\\xbe\\xd1\\x8d\\xd1\\x82\\xd0\\xbe\\xd0\\xbc\\xd1\\x83, \\xd0\\xbd\\xd0\\xb5 \\xd0\\xbc\\xd0\\xb5\\xd1\\x88\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0 \\xd0\\xbf\\xd0\\xb0\\xd1\\x81\\xd1\\x81\\xd0\\xb0\\xd0\\xb6\\xd0\\xb8\\xd1\\x80\\xd0\\xb0\\xd0\\xbc, \\xd0\\xbd\\xd0\\xbe \\xd0\\xba \\xd0\\xbf\\xd0\\xbe\\xd0\\xb5\\xd0\\xb7\\xd0\\xb4\\xd0\\xb0\\xd0\\xbc \\xd1\\x80\\xd0\\xb0\\xd0\\xb7\\xd1\\x80\\xd0\\xb5\\xd1\\x88\\xd0\\xb5\\xd0\\xbd\\xd0\\xbe \\xd0\\xbf\\xd0\\xbe\\xd0\\xb4\\xd1\\x8a\\xd0\\xb5\\xd0\\xb7\\xd0\\xb6\\xd0\\xb0\\xd1\\x82\\xd1\\x8c \\xd1\\x82\\xd0\\xbe\\xd0\\xbb\\xd1\\x8c\\xd0\\xba\\xd0\\xbe \\xd1\\x81\\xd0\\xbf\\xd0\\xb5\\xd1\\x86\\xd0\\xb8\\xd0\\xb0\\xd0\\xbb\\xd1\\x8c\\xd0\\xbd\\xd0\\xbe\\xd0\\xb9 \\xd1\\x82\\xd0\\xb5\\xd1\\x85\\xd0\\xbd\\xd0\\xb8\\xd0\\xba\\xd0\\xb5, \\xd0\\xbf\\xd0\\xbe\\xd1\\x8d\\xd1\\x82\\xd0\\xbe\\xd0\\xbc\\xd1\\x83 \\xd0\\xb4\\xd0\\xb5\\xd0\\xb9\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xb8\\xd1\\x8f \\xd1\\x80\\xd0\\xb0\\xd0\\xb1\\xd0\\xbe\\xd1\\x82\\xd0\\xbd\\xd0\\xb8\\xd0\\xba\\xd0\\xbe\\xd0\\xb2 \\xd0\\xb2\\xd0\\xbe\\xd0\\xba\\xd0\\xb7\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0 \\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb2\\xd0\\xb5\\xd1\\x80\\xd1\\x8f\\xd1\\x82\\xc2\\xbb '\n",
            " b'\\xd0\\xa1\\xd0\\xbe\\xd0\\xb2\\xd0\\xb5\\xd1\\x80\\xd1\\x88\\xd0\\xb5\\xd0\\xbd\\xd0\\xbd\\xd0\\xbe \\xd0\\xbe\\xd1\\x87\\xd0\\xb5\\xd0\\xb2\\xd0\\xb8\\xd0\\xb4\\xd0\\xbd\\xd0\\xbe, \\xd1\\x87\\xd1\\x82\\xd0\\xbe \\xd0\\xa2\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb7\\xd0\\xb0 \\xd0\\x9c\\xd1\\x8d\\xd0\\xb9 \\xd1\\x81\\xd0\\xb4\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0 \\xd0\\xb2\\xd1\\x81\\xd0\\xb5 \\xd0\\xbf\\xd1\\x80\\xd0\\xb5\\xd0\\xb6\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd0\\xbf\\xd0\\xbe\\xd0\\xb7\\xd0\\xb8\\xd1\\x86\\xd0\\xb8\\xd0\\xb8. \\xd0\\x95\\xd0\\xb5 \\xd0\\xbc\\xd0\\xb8\\xd0\\xbd\\xd0\\xb8\\xd1\\x81\\xd1\\x82\\xd1\\x80\\xd1\\x8b \\xd0\\xb2\\xd1\\x8b\\xd0\\xbf\\xd1\\x80\\xd1\\x8b\\xd0\\xb3\\xd0\\xb8\\xd0\\xb2\\xd0\\xb0\\xd1\\x8e\\xd1\\x82 \\xd0\\xb7\\xd0\\xb0 \\xd0\\xb1\\xd0\\xbe\\xd1\\x80\\xd1\\x82, \\xd0\\xb0 \\xd1\\x81\\xd0\\xb0\\xd0\\xbc\\xd0\\xbe \\xd0\\xbf\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xb8\\xd1\\x82\\xd0\\xb5\\xd0\\xbb\\xd1\\x8c\\xd1\\x81\\xd1\\x82\\xd0\\xb2\\xd0\\xbe \\xd1\\x82\\xd0\\xbe\\xd0\\xbd\\xd0\\xb5\\xd1\\x82'], shape=(5,), dtype=string)\n",
            "A batch of features (entity_text): tf.Tensor(\n",
            "[b'\\xd0\\x9d\\xd0\\xb8\\xd0\\xba\\xd0\\xb8\\xd1\\x82\\xd0\\xb0 \\xd0\\xa1\\xd0\\xb5\\xd1\\x80\\xd0\\xb3\\xd0\\xb5\\xd0\\xb5\\xd0\\xb2\\xd0\\xb8\\xd1\\x87'\n",
            " b'\\xd0\\x94\\xd0\\xb5 \\xd0\\xa5\\xd0\\xb5\\xd0\\xb0'\n",
            " b'\\xd0\\xa2\\xd0\\xb8\\xd1\\x82\\xd0\\xbe\\xd0\\xb2'\n",
            " b'\\xd0\\x90\\xd0\\xbb\\xd0\\xbb\\xd1\\x8b \\xd0\\x9f\\xd1\\x83\\xd0\\xb3\\xd0\\xb0\\xd1\\x87\\xd0\\xb5\\xd0\\xb2\\xd0\\xbe\\xd0\\xb9'\n",
            " b'\\xd0\\xa2\\xd0\\xb5\\xd1\\x80\\xd0\\xb5\\xd0\\xb7\\xd0\\xb0 \\xd0\\x9c\\xd1\\x8d\\xd0\\xb9'], shape=(5,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN0rZjAJxe2P"
      },
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = labse_preprocessor \n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = labse_encoder\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9EYxg1XpeKA",
        "outputId": "1d61b2d6-4bfd-47c5-fc9b-bf4c4cb6e400"
      },
      "source": [
        "classifier_model = build_classifier_model()\n",
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(tf.sigmoid(bert_raw_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.48670408]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhxFSL1ozegn"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "-lFXK0bLzfNz",
        "outputId": "5d45d0b3-b23b-405d-98f4-f70db352ee73"
      },
      "source": [
        "epochs = 5\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-ccf87087df35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_train_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOTWj4ILzhsi",
        "outputId": "aeb60b94-0ca5-4a39-f53a-9376dbd5b5be"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "data = [\n",
        "    \"ξεῖν᾽, ἦ τοι μὲν ὄνειροι ἀμήχανοι ἀκριτόμυθοι\",\n",
        "    \"γίγνοντ᾽, οὐδέ τι πάντα τελείεται ἀνθρώποισι.\",\n",
        "    \"δοιαὶ γάρ τε πύλαι ἀμενηνῶν εἰσὶν ὀνείρων:\",\n",
        "    \"αἱ μὲν γὰρ κεράεσσι τετεύχαται, αἱ δ᾽ ἐλέφαντι:\",\n",
        "    \"τῶν οἳ μέν κ᾽ ἔλθωσι διὰ πριστοῦ ἐλέφαντος,\",\n",
        "    \"οἵ ῥ᾽ ἐλεφαίρονται, ἔπε᾽ ἀκράαντα φέροντες:\",\n",
        "    \"οἱ δὲ διὰ ξεστῶν κεράων ἔλθωσι θύραζε,\",\n",
        "    \"οἵ ῥ᾽ ἔτυμα κραίνουσι, βροτῶν ὅτε κέν τις ἴδηται.\",\n",
        "]\n",
        "layer = preprocessing.TextVectorization()\n",
        "layer.adapt(data)\n",
        "vectorized_text = layer(data)\n",
        "print(vectorized_text)\n",
        "\n",
        "StringLookup"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[37 12 25  5  9 20 21  0  0]\n",
            " [51 34 27 33 29 18  0  0  0]\n",
            " [49 52 30 31 19 46 10  0  0]\n",
            " [ 7  5 50 43 28  7 47 17  0]\n",
            " [24 35 39 40  3  6 32 16  0]\n",
            " [ 4  2 15 14 22 23  0  0  0]\n",
            " [36 48  6 38 42  3 45  0  0]\n",
            " [ 4  2 13 41 53  8 44 26 11]], shape=(8, 9), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}